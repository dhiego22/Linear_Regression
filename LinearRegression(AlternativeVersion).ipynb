{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imports to analyse the data\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visualizing the Summary_of_Weather.csv' data\n",
    "\"\"\"\n",
    "data_weather = pd.read_csv('Summary_of_Weather.csv')\n",
    "print(data_weather.head())\n",
    "print(data_weather.tail())\n",
    "data_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visualizing the Weather_Station_locations.csv' data\n",
    "\"\"\"\n",
    "data_stations = pd.read_csv('Weather_Station_Locations.csv')\n",
    "print(data_stations.head())\n",
    "print(data_stations.tail())\n",
    "data_stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merging the data by stations\n",
    "\"\"\"\n",
    "data = data_weather.merge(data_stations,left_on='STA', right_on='WBAN')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ploting histogram of the data\n",
    "\"\"\"\n",
    "data.hist(bins=50, figsize=(8,6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ploting stations to visualize their locations\n",
    "\"\"\"\n",
    "data.plot(kind='scatter', x = 'Longitude', y = 'Latitude', alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check data correlations\n",
    "\"\"\"\n",
    "sns.heatmap(data.corr(), annot = True, fmt = '.2f')\n",
    "corr_matrix = data.corr()\n",
    "print(corr_matrix['MaxTemp'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Eliminating low correlated data and columns without significance\n",
    "\"\"\"\n",
    "data = data.drop(['Date','PoorWeather', 'FT', 'FB','FTI', 'ITH', 'SD3', 'RHX','RHN','RVG', 'WTE', 'SND', \n",
    "                  'TSHDSBRSGF','STA', 'YR', 'MO', 'PGT', 'DR', 'DA', 'SPD', 'WindGustSpd', 'Precip',  'MinTemp'\n",
    "                  ,'MeanTemp' , 'MaxTemp','Snowfall', 'SNF', 'PRCP', 'WBAN', 'ELEV', 'Longitude'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(ALTERNATIVE TO IMPUTER)\n",
    "Function to clean columns with zeros and NaN \n",
    "\"\"\"\n",
    "def clean_col_NaN(data_col)\n",
    "    unique_col = data_col.dropna().unique()\n",
    "    uni = pd.to_numeric(unique_col, float)\n",
    "    uni_mean = np.nanmean(uni)\n",
    "    data_col = pd.to_numeric(data_col, float)\n",
    "    data_col = data_col.fillna(uni_mean)\n",
    "    return data_col\n",
    "\n",
    "data.MIN = clean_col_NaN(data.MIN)#limpando coluna MIN\n",
    "data.MAX = clean_col_NaN(data.MAX)#limpando coluna MAX\n",
    "data.MEA = clean_col_NaN(data.MEA)#limpando coluna MEA\n",
    "data.Latitude = clean_col_NaN(data.Latitude)#limpando coluna Latitude\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preparing data to ML algorithms\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data,\n",
    "                                        test_size = 0.2,\n",
    "                                        random_state=35)\n",
    "print(\"data has {} atributes\\n {} train instances\\n {} test instances\".format\n",
    "      (len(data), len(train_set), len(test_set)))\n",
    "\n",
    "train_X = train_set.drop(['MAX'], axis=1)\n",
    "train_y = train_set.MaxTemp.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choosing and running the model\n",
    "\"\"\"\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from future_encoders import OneHotEncoder\n",
    "\n",
    "num_atribs = list(train_X.columns)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "some_data = train_x.iloc[:5]\n",
    "some_labels = train_y.iloc[:5]\n",
    "\n",
    "print(\"Predictions: \", lin_reg.predict(some_data))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = lin_reg.predict(some_data)\n",
    "lin_mse = mean_squared_error(some_labels, predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
